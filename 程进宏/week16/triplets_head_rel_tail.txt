BERT（谷歌2018年发布的预训练模型）	提出者	Google团队
Transformer模型	应用于	机器翻译
GPT_3	生成	自然语言文本
Word2Vec	改进	GloVe
斯坦福CoreNLP	包含功能	命名实体识别
spaCy	支持语言	英语_中文
ALBERT	基于	BERT
ELMo	使用	双向LSTM
T5模型	获奖	最佳论文奖
TextRank算法	应用于	文本摘要
BERT（谷歌2018年发布的预训练模型）	衍生模型	RoBERTa
Transformer模型	优化	多头注意力机制
GPT_3	开发机构	OpenAI
Word2Vec	训练方式	Skip_gram
斯坦福CoreNLP	开发机构	斯坦福大学
spaCy	主要用于	信息抽取
ALBERT	改进点	参数共享
ELMo	提出机构	艾伦人工智能研究所
T5模型	特点	文本到文本转换
TextRank算法	灵感来源	PageRank算法
BERT	属于	NLP领域
BERT	属于	预训练模型
Transformer模型	属于	NLP领域
GPT_3	属于	NLP领域
Word2Vec	属于	NLP领域
斯坦福CoreNLP	属于	NLP领域
spaCy	属于	NLP领域
ALBERT	属于	NLP领域
ELMo	属于	NLP领域
T5模型	属于	NLP领域
TextRank算法	属于	NLP领域